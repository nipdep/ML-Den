{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep cross-feature network (DCN)\n",
    "__Deep cross-feature network (DCN) build by the `tfrs` library__ \\\n",
    "[Source]() | [Dataset]() \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The handle \"movie_lens\" for the MovieLens dataset is deprecated. Prefer using \"movielens\" instead.\n",
      "2022-06-06 09:48:07.893745: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ratings = tfds.load(\"movie_lens/100k-ratings\", split=\"train\")\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_id\": x[\"movie_id\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"user_rating\"],\n",
    "    \"user_gender\": int(x[\"user_gender\"]),\n",
    "    \"user_zip_code\": x[\"user_zip_code\"],\n",
    "    \"user_occupation_text\": x[\"user_occupation_text\"],\n",
    "    \"bucketized_user_age\": int(x[\"bucketized_user_age\"]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"movie_id\", \"user_id\", \"user_gender\", \"user_zip_code\",\n",
    "                 \"user_occupation_text\", \"bucketized_user_age\"]\n",
    "\n",
    "vocabularies = {}\n",
    "\n",
    "for feature_name in feature_names:\n",
    "  vocab = ratings.batch(1_000_000).map(lambda x: x[feature_name])\n",
    "  vocabularies[feature_name] = np.unique(np.concatenate(list(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCN(tfrs.Model):\n",
    "\n",
    "  def __init__(self, use_cross_layer, deep_layer_sizes, projection_dim=None):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding_dimension = 32\n",
    "\n",
    "    str_features = [\"movie_id\", \"user_id\", \"user_zip_code\",\n",
    "                    \"user_occupation_text\"]\n",
    "    int_features = [\"user_gender\", \"bucketized_user_age\"]\n",
    "\n",
    "    self._all_features = str_features + int_features\n",
    "    self._embeddings = {}\n",
    "\n",
    "    # Compute embeddings for string features.\n",
    "    for feature_name in str_features:\n",
    "      vocabulary = vocabularies[feature_name]\n",
    "      self._embeddings[feature_name] = tf.keras.Sequential(\n",
    "          [tf.keras.layers.StringLookup(\n",
    "              vocabulary=vocabulary, mask_token=None),\n",
    "           tf.keras.layers.Embedding(len(vocabulary) + 1,\n",
    "                                     self.embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for int features.\n",
    "    for feature_name in int_features:\n",
    "      vocabulary = vocabularies[feature_name]\n",
    "      self._embeddings[feature_name] = tf.keras.Sequential(\n",
    "          [tf.keras.layers.IntegerLookup(\n",
    "              vocabulary=vocabulary, mask_value=None),\n",
    "           tf.keras.layers.Embedding(len(vocabulary) + 1,\n",
    "                                     self.embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    if use_cross_layer:\n",
    "      self._cross_layer = tfrs.layers.dcn.Cross(\n",
    "          projection_dim=projection_dim,\n",
    "          kernel_initializer=\"glorot_uniform\")\n",
    "    else:\n",
    "      self._cross_layer = None\n",
    "\n",
    "    self._deep_layers = [tf.keras.layers.Dense(layer_size, activation=\"relu\")\n",
    "      for layer_size in deep_layer_sizes]\n",
    "\n",
    "    self._logit_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.RootMeanSquaredError(\"RMSE\")]\n",
    "    )\n",
    "\n",
    "  def call(self, features):\n",
    "    # Concatenate embeddings\n",
    "    embeddings = []\n",
    "    for feature_name in self._all_features:\n",
    "      embedding_fn = self._embeddings[feature_name]\n",
    "      embeddings.append(embedding_fn(features[feature_name]))\n",
    "\n",
    "    x = tf.concat(embeddings, axis=1)\n",
    "\n",
    "    # Build Cross Network\n",
    "    if self._cross_layer is not None:\n",
    "      x = self._cross_layer(x)\n",
    "\n",
    "    # Build Deep Network\n",
    "    for deep_layer in self._deep_layers:\n",
    "      x = deep_layer(x)\n",
    "\n",
    "    return self._logit_layer(x)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    labels = features.pop(\"user_rating\")\n",
    "    scores = self(features)\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=scores,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "learning_rate = 0.01\n",
    "\n",
    "def run_models(use_cross_layer, deep_layer_sizes, projection_dim=None, num_runs=5):\n",
    "  models = []\n",
    "  rmses = []\n",
    "\n",
    "  for i in range(num_runs):\n",
    "    model = DCN(use_cross_layer=use_cross_layer,\n",
    "                deep_layer_sizes=deep_layer_sizes,\n",
    "                projection_dim=projection_dim)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "    models.append(model)\n",
    "\n",
    "    model.fit(cached_train, epochs=epochs, verbose=False)\n",
    "    metrics = model.evaluate(cached_test, return_dict=True)\n",
    "    rmses.append(metrics[\"RMSE\"])\n",
    "\n",
    "  mean, stdv = np.average(rmses), np.std(rmses)\n",
    "\n",
    "  return {\"model\": models, \"mean\": mean, \"stdv\": stdv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:mask_value is deprecated, use mask_token instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:mask_value is deprecated, use mask_token instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 26ms/step - RMSE: 0.9306 - loss: 0.8662 - regularization_loss: 0.0000e+00 - total_loss: 0.8662\n",
      "5/5 [==============================] - 0s 6ms/step - RMSE: 0.9339 - loss: 0.8725 - regularization_loss: 0.0000e+00 - total_loss: 0.8725\n",
      "5/5 [==============================] - 0s 5ms/step - RMSE: 0.9325 - loss: 0.8700 - regularization_loss: 0.0000e+00 - total_loss: 0.8700\n",
      "5/5 [==============================] - 0s 5ms/step - RMSE: 0.9350 - loss: 0.8751 - regularization_loss: 0.0000e+00 - total_loss: 0.8751\n",
      "5/5 [==============================] - 0s 6ms/step - RMSE: 0.9339 - loss: 0.8729 - regularization_loss: 0.0000e+00 - total_loss: 0.8729\n"
     ]
    }
   ],
   "source": [
    "dcn_result = run_models(use_cross_layer=True,\n",
    "                        deep_layer_sizes=[192, 192])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "114915f1d81c1da50642110d3f97d7e46f1e3067055e32d41ec3bf538df62b4a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('rec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
